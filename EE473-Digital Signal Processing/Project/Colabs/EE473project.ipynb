{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EE473project.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lX2EioNcIgB2","colab_type":"code","colab":{}},"source":["import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","import pickle\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torchvision\n","from google.colab import drive, files"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cR73BQiUEXDO","colab_type":"code","colab":{}},"source":["epochs = 1\n","batch_size = 50\n","lr = 1e-4\n","\n","num_classes = 5\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0TPngXnLEiJb","colab_type":"code","colab":{}},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(torch.cuda.get_device_name(0))\n","print(device)\n","\n","from google.colab import drive\n","drive.mount('/drive', force_remount=True)\n","path = '/drive/My Drive/BaumeisterAI/Colabs/envelopeDetection/'\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGFGwM8FI4a-","colab_type":"code","colab":{}},"source":["model = torchvision.models.resnet18(pretrained=False, progress=True)\n","num_classes = 5\n","model.fc = nn.Linear(512, num_classes)\n","\n","model.train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4Jo5n16I-Ge","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","%tensorboard --logdir logs/tensorboard\n","\n","from tensorboardcolab import TensorBoardColab\n","tb = TensorBoardColab()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6vAsdeDFNXd4","colab_type":"code","colab":{}},"source":["optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","#optimizer = optim.Adam(model.parameters(), lr=lr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKqmcDOMKRSf","colab_type":"code","colab":{}},"source":["model.train()\n","for epoch in range(epochs):\n","    \n","    running_loss = 0.0\n","    running_loss_classifier = 0.0\n","    running_loss_box_reg = 0.0\n","    running_loss_objectness = 0.0\n","    running_loss_rpn_box_reg = 0.0\n","        \n","    for i in range(1750, len(training_images)):\n","      \n","        optimizer.zero_grad()\n","\n","        image = [torch.from_numpy(training_images[i]).view(1, 256, 256).float().cuda()]\n","        targets = [{'labels':training_targets[i]['labels'].cuda(), 'boxes':training_targets[i]['boxes'].cuda()}]\n","        outputs = model(image, targets)\n","        \n","        loss_classifier = outputs['loss_classifier']\n","        loss_box_reg = outputs['loss_box_reg'] \n","        loss_objectness = outputs['loss_objectness']\n","        loss_rpn_box_reg = outputs['loss_rpn_box_reg'] * 1e-1\n","        \n","        running_loss_classifier += loss_classifier\n","        running_loss_box_reg += loss_box_reg\n","        running_loss_objectness += loss_objectness\n","        running_loss_rpn_box_reg += loss_rpn_box_reg\n","        \n","        loss = loss_classifier + loss_box_reg + loss_objectness + loss_rpn_box_reg     \n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss\n","        if i%50 == 49:\n","            print(\"Epoch:{}, Step:{}, Loss:{}\".format(epoch+1, i+1, running_loss/50))\n","            running_loss = 0.0\n","        \n","        universal_step = i + len(training_images)*total_pass\n","        tb.save_value('Train Loss', 'total_loss', universal_step, loss.item())\n","        tb.save_value('Train Loss', 'loss_classifier', universal_step, loss_classifier)\n","        tb.save_value('Train Loss', 'loss_box_reg', universal_step, loss_box_reg)\n","        tb.save_value('Train Loss', 'loss_objectness', universal_step, loss_objectness)\n","        tb.save_value('Train Loss', 'loss_rpn_box_reg', universal_step, loss_rpn_box_reg)\n","\n","total_pass += 1        \n","\n","print('Finished training.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QP5ILFm22JZG","colab_type":"code","colab":{}},"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class MusicnetComposers(Dataset):\n","    def __init__(self, csv_path, dataset_path, composers, split_duration, n_samples, fs):\n","        musicnet_metadata = pd.read_csv(csv_path)\n","        with open(dataset_path, 'rb') as npz:\n","          musicnet_dataset = np.load(dataset_path, encoding = 'latin1', allow_pickle=True)\n","        self.dataset = np.zeros((len(composers)*n_samples, fs*split_duration))\n","        self.labels = np.zeros(len(composers)*n_samples, dtype=int)\n","        count = 0\n","        for composer in composers:\n","          composerMetadata = musicnet_metadata.loc[musicnet_metadata.composer == composer]\n","          composer_data = []\n","          for row in composerMetadata.itertuples():\n","            print(row.Index)\n","            id = str(row.id)\n","            duration = row.seconds\n","            sound, _ = musicnet_dataset[id]\n","            n_splits = np.floor(duration/split_duration).astype('int64')\n","            for i in range(n_splits):\n","              start = i*fs*split_duration\n","              end = (i+1)*fs*split_duration\n","              split = sound[start:end]\n","              composer_data.append(split)\n","            if len(composer_data) >= n_samples:\n","              break\n","          index = np.random.randint(low=0, high=len(composer_data), size=n_samples)\n","          composer_data = [composer_data[i] for i in index]\n","          print(len(composer_data))\n","          print(np.array(composer_data).shape)\n","          print(self.dataset[count*n_samples:(count+1)*n_samples].shape)\n","\n","          self.dataset[count*n_samples:(count+1)*n_samples] = np.array(composer_data)\n","          self.labels[count*n_samples:(count+1)*n_samples] = count\n","          count += 1\n","          if len(composer_data) >= n_samples:\n","            break\n","\n","        self.composers = composers\n","        self.split_duration = split_duration\n","        self.n_samples = n_samples\n","        self.fs = fs\n","        \n","    def __getitem__(self, index):\n","\n","        sound = torch.tensor(self.dataset[index]).reshape(1,-1)\n","        spectogram = MelSpectrogram(sample_rate=self.fs, \\\n","                                     n_fft=2048, \\\n","                                     win_length=None, \\\n","                                     hop_length=512, \\\n","                                     f_min=0.0, \\\n","                                     f_max=None, \\\n","                                     pad=0, \\\n","                                     n_mels=128, \\\n","                                     \\\n","                                     )(sound)\n","      \n","        return self.dataset[index], self.labels[index]\n","    \n","    def __len__(self):\n","        return len(self.labels)\n","\n","    \n","csv_path = 'musicnet_metadata.csv'\n","dataset_path = 'musicnet.npz'\n","fs = 44100\n","composers = ['Schubert', 'Beethoven', 'Brahms', 'Mozart', 'Bach']\n","split_duration = 20\n","n_samples = 100\n","train_set = MusicnetComposers(csv_path, dataset_path, composers, split_duration, n_samples, fs)\n","print(\"Train set size: \" + str(len(train_set)))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hx_0Koqb1uf6","colab_type":"code","colab":{}},"source":["optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IndGQ2lY1vYX","colab_type":"code","colab":{}},"source":["def train(model, epoch):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        data = data.to(device)\n","        target = target.to(device)\n","        data = data.requires_grad_() #set requires_grad to True for training\n","        output = model(data)\n","        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n","        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % log_interval == 0: #print training stats\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7dpBjlsb11Qd","colab_type":"code","colab":{}},"source":["def test(model, epoch):\n","    model.eval()\n","    correct = 0\n","    for data, target in test_loader:\n","        data = data.to(device)\n","        target = target.to(device)\n","        output = model(data)\n","        output = output.permute(1, 0, 2)\n","        pred = output.max(2)[1] # get the index of the max log-probability\n","        correct += pred.eq(target).cpu().sum().item()\n","    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kG_QUN1_13gB","colab_type":"code","colab":{}},"source":["log_interval = 20\n","for epoch in range(1, 41):\n","    if epoch == 31:\n","        print(\"First round of training complete. Setting learn rate to 0.001.\")\n","    scheduler.step()\n","    train(model, epoch)\n","    test(model, epoch)"],"execution_count":0,"outputs":[]}]}