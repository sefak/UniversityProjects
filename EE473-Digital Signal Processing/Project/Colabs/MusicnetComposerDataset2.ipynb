{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MusicnetComposerDataset2.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Y1PYDAswToTO","colab_type":"code","colab":{}},"source":["import torch\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(torch.cuda.get_device_name(0))\n","print(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"064rMXhWLpGU","colab_type":"code","colab":{}},"source":["!wget https://homes.cs.washington.edu/~thickstn/media/musicnet.npz\n","!wget https://homes.cs.washington.edu/~thickstn/media/musicnet_metadata.csv\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CFQi3mkW5LnE","colab_type":"code","colab":{}},"source":["!sudo apt-get install sox libsox-dev libsox-fmt-all\n","!pip install git+git://github.com/pytorch/audio"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M67rsahJLect","colab_type":"code","colab":{}},"source":["import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","import pickle\n","from google.colab import drive, files\n","import pandas as pd\n","import torch\n","import librosa\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n","from torchvision.datasets import ImageFolder\n","import torchvision\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn as nn\n","from scipy.signal import spectrogram"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXXqHDPmLm1k","colab_type":"code","colab":{}},"source":["fs = 44100\n","composers = ['Schubert', 'Beethoven', 'Brahms', 'Mozart', 'Bach']\n","segment_duration = 20\n","n_samples = 20\n","train_test_partition = 0.8\n","metadata_path = 'musicnet_metadata.csv'\n","dataset_path = 'musicnet.npz'\n","\n","transform_params = {'sample_rate':fs, \\\n","                    'n_fft':4096, \\\n","                    'win_length':None, \\\n","                    'hop_length':512, \\\n","                    'f_min':0.0, \\\n","                    'f_max':5000, \\\n","                    'pad':0, \\\n","                    'n_mels':128, \\\n","                    }\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrYr8iMVL1f6","colab_type":"code","colab":{}},"source":["class MusicnetComposers(Dataset):\n","    def __init__(self, csv_path, dataset_path, composers, segment_duration, n_samples, fs, transform):\n","        musicnet_metadata = pd.read_csv(csv_path) #Load metada file \n","        with open(dataset_path, 'rb') as npz: #Load sound dataset\n","          musicnet_dataset = np.load(dataset_path, encoding = 'latin1', allow_pickle=True)\n","\n","\n","        self.transform = MelSpectrogram(sample_rate=fs, \\\n","                                        n_fft = transform['n_fft'], \\\n","                                        win_length = transform['win_length'], \\\n","                                        hop_length = transform['hop_length'], \\\n","                                        f_min = transform['f_min'], \\\n","                                        f_max = transform['f_max'], \\\n","                                        pad = transform['pad'], \\\n","                                        n_mels = transform['n_mels'], \\\n","                                        )\n","\n","        #Allocate arrays\n","        #self.dataset = torch.zeros((len(composers)*n_samples, \\\n","        #                            1, \\\n","        #                         transform['n_mels'], \\\n","        #                         np.ceil(segment_duration*fs/transform['hop_length']).astype('int64')\\\n","        #                        ))\n","        self.dataset = torch.zeros((300, 1, 129, 3937))\n","        self.labels = torch.zeros(len(composers)*n_samples, dtype=int)\n","\n","        for composer_id, composer in enumerate(composers): #Do for each composer\n","\n","          \n","          composerData = musicnet_metadata.composer == composer #Locate the data of a composer\n","          composerMetadata = musicnet_metadata.loc[composerData] #Extract data related to a composer\n","          composer_data = [] #Temporary sound list\n","\n","          for row in composerMetadata.itertuples(): #Do for each music of a composer\n","            \n","            #Extract information related to a music\n","            id = str(row.id)\n","            duration = row.seconds\n","            sound, _ = musicnet_dataset[id]\n","            n_splits = np.floor(duration/segment_duration).astype('int64')\n","\n","            for i in range(n_splits): #Do for each segment of a music\n","              start = i*fs*segment_duration #Starting time of a music segment\n","              end = (i+1)*fs*segment_duration #Ending time of a music segment\n","              segment = sound[start:end]   #Extract a segment\n","              #segment = torch.Tensor(segment).reshape(1,-1)\n","              #segment = self.transform(segment)\n","              _, _, segment = spectrogram(segment, fs)\n","              #segment = AmplitudeToDB('power', top_db = 80)(segment)\n","              #segment = segment - 10 * torch.log10(torch.max(segment))\n","              segment = np.log(segment)\n","              segment = torch.Tensor(segment).unsqueeze(0)\n","              composer_data.append(segment)\n","              if len(composer_data)>=n_samples: break\n","            if len(composer_data)>=n_samples: break\n","          #Create a random index to select n_samples many samples for a composer\n","          index = np.arange(0, len(composer_data), 1)\n","          index = np.random.choice(index, size=n_samples, replace=False)\n","          #index = np.arange(0, n_samples, 1)\n","          composer_data = [composer_data[i] for i in index]\n","\n","          #Create index vector to add a composer data to MusicnetComposers dataset\n","          index = np.arange(composer_id*n_samples, (composer_id+1)*n_samples, 1)\n","\n","          #Add a composer data to MusicnetComposers dataset\n","          self.dataset[index] = torch.stack(composer_data)\n","          self.labels[index] = composer_id\n","          print('Composer {} is done.'.format(composer))\n","        #Save hyper-parameters\n","        self.composers = composers\n","        self.segment_duration = segment_duration\n","        self.n_samples = n_samples\n","        self.fs = fs\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","\n","        #Get sound from original Musicnet dataset and convert it to torch.Tensor\n","        #Also each sound sample needs to be of the form (channel, time), so reshape it\n","\n","        #Obtain spectogram of each sound\n","      \n","        return self.dataset[index], self.labels[index]\n","    \n","    def __len__(self):\n","        return len(self.labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYmwRn1mmbuJ","colab_type":"code","colab":{}},"source":["musicnet_metadata = pd.read_csv(metadata_path) #Load metada file \n","with open(dataset_path, 'rb') as npz: #Load sound dataset\n","  musicnet_dataset = np.load(dataset_path, encoding = 'latin1', allow_pickle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uFvpMyfFj32p","colab_type":"code","colab":{}},"source":["#Allocate arrays\n","#self.dataset = torch.zeros((len(composers)*n_samples, \\\n","#                            1, \\\n","#                         transform['n_mels'], \\\n","#                         np.ceil(segment_duration*fs/transform['hop_length']).astype('int64')\\\n","#                        ))\n","dataset = []\n","labels = []\n","\n","for composer_id, composer in enumerate(composers): #Do for each composer\n","\n","  \n","  composerData = musicnet_metadata.composer == composer #Locate the data of a composer\n","  composerMetadata = musicnet_metadata.loc[composerData] #Extract data related to a composer\n","  composer_data = [] #Temporary sound list\n","\n","  for row in composerMetadata.itertuples(): #Do for each music of a composer\n","    \n","    #Extract information related to a music\n","    id = str(row.id)\n","    duration = row.seconds\n","    sound, _ = musicnet_dataset[id]\n","\n","    _, _, sound = spectrogram(sound, fs)\n","    sound = np.log(sound)\n","    sound = torch.Tensor(sound).unsqueeze(0)\n","    composer_data.append(sound)\n","    if len(composer_data)>=n_samples: break\n","  #Create a random index to select n_samples many samples for a composer\n","  index = np.arange(0, len(composer_data), 1)\n","  index = np.random.choice(index, size=n_samples, replace=False)\n","  composer_data = [composer_data[i] for i in index]\n","\n","  #Create index vector to add a composer data to MusicnetComposers dataset\n","  index = np.arange(0, n_samples, 1)\n","  print(index)\n","\n","  \n","  [dataset.append(composer_data[i]) for i in index]\n","  print(composer_id)\n","  print(labels)\n","  [labels.append(composer_id) for i in index]\n","  print(labels)\n","\n","  print('Composer {} is done.'.format(composer))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKu5KtWkRn8z","colab_type":"code","colab":{}},"source":["composerDataset = MusicnetComposers(metadata_path, \\\n","                                    dataset_path, \\\n","                                    composers, \\\n","                                    segment_duration, \\\n","                                    24, \\\n","                                    fs, \\\n","                                    transform_params \\\n","                                    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ji_rLLOEu2vE","colab_type":"code","colab":{}},"source":["labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwE5YSj20oL6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQPBdioKyneo","colab_type":"code","colab":{}},"source":["train_len = int(train_test_partition * len(composers) * n_samples)\n","test_len = len(composers) * n_samples - train_len\n","train_set, test_set = random_split(composerDataset, [1200, 300])\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size = 8, shuffle = True)\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size = 1, shuffle = True)\n","\n","dataset_loader = torch.utils.data.DataLoader(composerDataset, batch_size = 8, shuffle = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ST8A5IU79bG3","colab_type":"code","colab":{}},"source":["model = torchvision.models.resnet34(pretrained=False, progress=True)\n","num_classes = 5\n","model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","model.fc = nn.Linear(512, num_classes)\n","\n","model = nn.Sequential(\n","    model,\n","    nn.Softmax(1)\n",")\n","\n","model.train()\n","model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mWBwgbN_Dhh","colab_type":"code","colab":{}},"source":["optimizer = optim.SGD(model.parameters(), lr = 0.001, weight_decay = 0.0001)\n","#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mw1pGJsFAdQ_","colab_type":"code","colab":{}},"source":["def train(model, epoch, dataset_loader):\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(dataset_loader):\n","        optimizer.zero_grad()\n","        data = data.to(device)\n","        target = target.to(device)\n","        data = data.requires_grad_() #set requires_grad to True for training\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % log_interval == 0: #print training stats\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRGkyIHklvJL","colab_type":"code","colab":{}},"source":["  model.train()\n","  for batch_idx, data in enumerate(dataset):\n","      target = torch.tensor(labels[batch_idx]).unsqueeze(0).float()\n","      data = data.unsqueeze(0).float()\n","      optimizer.zero_grad()\n","      data = data.to(device)\n","      target = target.to(device)\n","      data = data.requires_grad_() #set requires_grad to True for training\n","      output = model(data)\n","      loss = F.nll_loss(output, target)\n","      loss.backward()\n","      optimizer.step()\n","      if batch_idx % log_interval == 0: #print training stats\n","          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","              epoch, batch_idx * len(data), len(train_loader.dataset),\n","              100. * batch_idx / len(train_loader), loss))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MnlXYjEFojV6","colab_type":"code","colab":{}},"source":["data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PQqj_BG0mUsp","colab_type":"code","colab":{}},"source":["!nvidia-smi\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHcPhepiAdUp","colab_type":"code","colab":{}},"source":["def test(model, epoch):\n","    model.eval()\n","    correct = 0\n","    for data, target in test_loader:\n","        data = data.to(device)\n","        target = target.to(device)\n","        output = model(data)[0]\n","        _, pred = output.max(0)\n","        correct += pred.eq(target).cpu().sum().item()\n","    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kttKI2mZdHXg","colab_type":"code","colab":{}},"source":["e = train(model, 1, train_loader)\n","e.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfSfdeO0AdaP","colab_type":"code","colab":{}},"source":["log_interval = 20\n","for epoch in range(1, 2):\n","    if epoch == 31:\n","        print(\"First round of training complete. Setting learn rate to 0.001.\")\n","    #scheduler.step()\n","    train(model, epoch, dataset_loader)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8s24m_BVAdjn","colab_type":"code","colab":{}},"source":["idx = np.random.randint(0, len(test_set))\n","\n","model.eval()\n","g, id = test_set[idx]\n","g = g.to(device)\n","print(id.item())\n","out, label = model(g.unsqueeze(0))[0].max(0)\n","print(out)\n","print(label.item())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WeVBcX_rSqbt","colab_type":"code","colab":{}},"source":["test(model, 1)[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"teUOe0gmKWd6","colab_type":"code","colab":{}},"source":["import librosa, librosa.display\n","idx = np.random.randint(0, len(composerDataset))\n","a = composerDataset[idx][0].squeeze()\n","#a = e[0].squeeze()\n","a.shape\n","a = a.squeeze()\n","plt.figure(figsize=(10, 4))\n","print(a.shape)\n","S_dB = AmplitudeToDB('power', top_db = 80)(a)\n","S_dB = S_dB.numpy()\n","#S_dB = librosa.power_to_db(a.numpy(), ref=np.max)\n","librosa.display.specshow(a.numpy(), x_axis='time', y_axis='mel', sr=fs)\n","plt.colorbar(format='%+2.0f dB')\n","plt.title('Mel-frequency spectrogram')\n","plt.tight_layout()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGpKRcmbKZl6","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ES2Q-I2OVJxL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tl4HtOpBZ6vL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"obSlCyKld7xE","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}