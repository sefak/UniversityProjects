{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MusicnetComposerDataset.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"064rMXhWLpGU","colab_type":"code","colab":{}},"source":["!wget https://homes.cs.washington.edu/~thickstn/media/musicnet.npz\n","!wget https://homes.cs.washington.edu/~thickstn/media/musicnet_metadata.csv\n","!sudo apt-get install sox libsox-dev libsox-fmt-all\n","!pip install git+git://github.com/pytorch/audio"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M67rsahJLect","colab_type":"code","colab":{}},"source":["import numpy as np\n","import cv2\n","import matplotlib\n","matplotlib.use('Agg')\n","import matplotlib.pyplot as plt\n","import os\n","import shutil\n","import pickle\n","from google.colab import drive, files\n","import pandas as pd\n","import torch\n","import librosa\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n","from torchvision.datasets import ImageFolder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXXqHDPmLm1k","colab_type":"code","colab":{}},"source":["fs = 44100\n","composers = ['Schubert', 'Beethoven', 'Brahms', 'Mozart', 'Bach']\n","segment_duration = 20\n","n_samples = 300\n","train_test_partition = 0.8\n","metadata_path = 'musicnet_metadata.csv'\n","dataset_path = 'musicnet.npz'\n","\n","transform_params = {'sample_rate':fs, \\\n","                    'n_fft':2048, \\\n","                    'win_length':None, \\\n","                    'hop_length':1024, \\\n","                    'f_min':0.0, \\\n","                    'f_max':None, \\\n","                    'pad':0, \\\n","                    'n_mels':512, \\\n","                    }\n","\n","save_path = '/drive/My Drive/Academic/EE473/composerDataset.dill'\n","save_image_path = '/drive/My Drive/Academic/EE473/specDataset.dill'\n","figure_path = 'data/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrYr8iMVL1f6","colab_type":"code","colab":{}},"source":["class MusicnetComposers(Dataset):\n","    def __init__(self, csv_path, dataset_path, composers, segment_duration, n_samples, fs, transform):\n","        musicnet_metadata = pd.read_csv(csv_path) #Load metada file \n","        with open(dataset_path, 'rb') as npz: #Load sound dataset\n","          musicnet_dataset = np.load(dataset_path, encoding = 'latin1', allow_pickle=True)\n","\n","\n","        self.transform = MelSpectrogram(sample_rate=fs, \\\n","                                        n_fft = transform['n_fft'], \\\n","                                        win_length = transform['win_length'], \\\n","                                        hop_length = transform['hop_length'], \\\n","                                        f_min = transform['f_min'], \\\n","                                        f_max = transform['f_max'], \\\n","                                        pad = transform['pad'], \\\n","                                        n_mels = transform['n_mels'], \\\n","                                        )\n","\n","        #Allocate arrays\n","        self.dataset = torch.zeros((len(composers)*n_samples, \\\n","                                    1, \\\n","                                 transform['n_mels'], \\\n","                                 np.ceil(segment_duration*fs/transform['hop_length']).astype('int64')\\\n","                                ))\n","        self.labels = torch.zeros(len(composers)*n_samples, dtype=int)\n","\n","        for composer_id, composer in enumerate(composers): #Do for each composer\n","\n","          \n","          composerData = musicnet_metadata.composer == composer #Locate the data of a composer\n","          composerMetadata = musicnet_metadata.loc[composerData] #Extract data related to a composer\n","          composer_data = [] #Temporary sound list\n","\n","          for row in composerMetadata.itertuples(): #Do for each music of a composer\n","\n","            #Extract information related to a music\n","            id = str(row.id)\n","            duration = row.seconds\n","            sound, _ = musicnet_dataset[id]\n","            n_splits = np.floor(duration/segment_duration).astype('int64')\n","\n","            for i in range(n_splits): #Do for each segment of a music\n","              start = i*fs*segment_duration #Starting time of a music segment\n","              end = (i+1)*fs*segment_duration #Ending time of a music segment\n","              segment = sound[start:end]   #Extract a segment\n","              segment = torch.Tensor(segment).reshape(1,-1)\n","              segment = self.transform(segment)\n","              segment = AmplitudeToDB('power', top_db = 80)(segment)\n","              segment = segment - 10 * torch.log10(torch.max(segment))\n","              composer_data.append(segment)\n","              #if len(composer_data)>=n_samples: break\n","          #Create a random index to select n_samples many samples for a composer\n","          index = np.arange(0, len(composer_data), 1)\n","          index = np.random.choice(index, size=n_samples, replace=False)\n","          #index = np.arange(0, n_samples, 1)\n","          composer_data = [composer_data[i] for i in index]\n","\n","          #Create index vector to add a composer data to MusicnetComposers dataset\n","          index = np.arange(composer_id*n_samples, (composer_id+1)*n_samples, 1)\n","\n","          #Add a composer data to MusicnetComposers dataset\n","          self.dataset[index] = torch.stack(composer_data)\n","          self.labels[index] = composer_id\n","          print('Composer {} is done.'.format(composer))\n","          #if len(composer_data)>=n_samples: break\n","        #Save hyper-parameters\n","        self.composers = composers\n","        self.segment_duration = segment_duration\n","        self.n_samples = n_samples\n","        self.fs = fs\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","\n","        #Get sound from original Musicnet dataset and convert it to torch.Tensor\n","        #Also each sound sample needs to be of the form (channel, time), so reshape it\n","\n","        #Obtain spectogram of each sound\n","      \n","        return self.dataset[index], self.labels[index]\n","    \n","    def __len__(self):\n","        return len(self.labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKu5KtWkRn8z","colab_type":"code","colab":{}},"source":["composerDataset = MusicnetComposers(metadata_path, \\\n","                                    dataset_path, \\\n","                                    composers, \\\n","                                    segment_duration, \\\n","                                    n_samples, \\\n","                                    fs, \\\n","                                    transform_params \\\n","                                    )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ji_rLLOEu2vE","colab_type":"code","colab":{}},"source":["import librosa, librosa.display\n","\n","a , _ = composerDataset[0]\n","a.shape\n","a = a.squeeze()\n","b = AmplitudeToDB('power', top_db = 80)(a)\n","b = b - 10 * torch.log10(torch.max(a))\n","plt.figure(figsize=(10, 4))\n","S_dB = librosa.power_to_db(a.numpy(), ref=np.max)\n","librosa.display.specshow(b.numpy(), x_axis='time', y_axis='mel', sr=fs)\n","plt.colorbar(format='%+2.0f dB')\n","plt.title('Mel-frequency spectrogram')\n","plt.tight_layout()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwE5YSj20oL6","colab_type":"code","colab":{}},"source":["torch.max(a).item()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IQPBdioKyneo","colab_type":"code","colab":{}},"source":["import librosa, librosa.display\n","\n","a , _ = composerDataset[0]\n","a.shape\n","a = a.squeeze()\n","b = AmplitudeToDB()(a)\n","plt.figure(figsize=(10, 4))\n","S_dB = librosa.power_to_db(a.numpy(), ref=np.max)\n","librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=fs)\n","plt.colorbar(format='%+2.0f dB')\n","plt.title('Mel-frequency spectrogram')\n","plt.tight_layout()\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ST8A5IU79bG3","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/drive', force_remount=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mWBwgbN_Dhh","colab_type":"code","colab":{}},"source":["import dill\n","with open(save_path, 'wb') as file:\n","  dill.dump(composerDataset, file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mw1pGJsFAdQ_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHcPhepiAdUp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfSfdeO0AdaP","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8s24m_BVAdjn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WeVBcX_rSqbt","colab_type":"code","colab":{}},"source":["if os.path.isdir(figure_path):\n","  shutil.rmtree(figure_path)\n","  os.mkdir(figure_path)\n","else:\n","  os.mkdir(figure_path)\n","\n","for composer in composers:\n","  composerFolder = figure_path+composer\n","  os.mkdir(composerFolder)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"teUOe0gmKWd6","colab_type":"code","colab":{}},"source":["for count, (spec, label) in enumerate(composerDataset):\n","  print(count)\n","  composer = composers[label]\n","  fig = plt.figure(figsize=(10,10))\n","  S_dB = librosa.power_to_db(spec, ref=np.max)\n","  plt.pcolormesh(S_dB, cmap = 'magma')\n","  plt.axis('off')\n","  spec_path = figure_path + composer + '/{}.png'.format(count)\n","  plt.savefig(spec_path, transparent = True, format = 'png')\n","  plt.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGpKRcmbKZl6","colab_type":"code","colab":{}},"source":["from torchvision.datasets import ImageFolder\n","specFigureData = ImageFolder(figure_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ES2Q-I2OVJxL","colab_type":"code","colab":{}},"source":["save_image_path = '/drive/My Drive/Academic/EE473/specDataset.dill'\n","import dill\n","with open(save_image_path, 'wb') as file:\n","  dill.dump(specFigureData, file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tl4HtOpBZ6vL","colab_type":"code","colab":{}},"source":["model(torchvision.transforms.ToTensor()(a))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"obSlCyKld7xE","colab_type":"code","colab":{}},"source":["model(b)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nw3GVEfnZ688","colab_type":"code","colab":{}},"source":["import torchvision\n","import torch.nn as nn\n","model = torchvision.models.resnet18(pretrained=False, progress=True)\n","num_classes = 5\n","model.fc = nn.Linear(512, num_classes)\n","\n","model.train()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrYSEDS3bUvk","colab_type":"code","colab":{}},"source":["a, _ =specFigureData[0]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FP8q_SwbcxD","colab_type":"code","colab":{}},"source":["b = torchvision.transforms.ToTensor()(a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5x3y81-obhmc","colab_type":"code","colab":{}},"source":["model.eval()\n","model(c.unsqueeze(0))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C0J251m3bm7k","colab_type":"code","colab":{}},"source":["import cv2\n","\n","c = torch.Tensor(cv2.resize(b.permute(1,2,0).numpy(), (224,224))).permute(2,0,1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtNgu3sZeFi7","colab_type":"code","colab":{}},"source":["\n","np.array(a).shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zbDGZo-2eICp","colab_type":"code","colab":{}},"source":["np.array(a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NXw2-Ev4eSIi","colab_type":"code","colab":{}},"source":["b.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQcPnRobez9V","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}